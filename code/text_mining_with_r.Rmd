---
title: "text_mining"
author: "Selvin Carrillo"
date: "created: November 17, 2022; updated: November 17, 2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE, warning=FALSE, message=FALSE}
# Clear workspace 
rm(list = ls())

# Load libraries 
library(tidyverse) 
library(tidytext)
library(janeaustenr)
```

## Tidy text 
unnest_tokens 
```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text_df <- 
     tibble(lines = 1:4, 
            text = text)

text_df %>% 
     unnest_tokens(word, token = "words", text)
```

Try to do some work with my thesis document 
```{r}
library(pdftools)

# find .pdf files in my directory 
thesis <- list.files(pattern = ".pdf$")

map_df(thesis, ~ data.frame(txt = pdf_text(.x))) %>% 
     mutate(filename = .x) %>% 
     unnest_tokens(word, txt)


```

Tidying text 
```{r}


austen_books() %>% 
     group_by(book) %>% 
     mutate(linenumber = row_number(), 
            chapter = cumsum(str_detect(text, 
                                        regex("^chapter [\\divxlc]", 
                                              ignore_case = TRUE)))) %>% 
     ungroup() %>% 
     # one-token-per-row
     unnest_tokens(word, text) %>% 
     # remove not useful words (stop words)
     anti_join(stop_words) %>%
     # most common words
     count(word,sort = TRUE) %>% 
     filter(n > 600) %>% 
     mutate(word = reorder(word, n)) %>% 
     
     # plot of most common words 
     ggplot(aes(n, word)) + 
     geom_col() + 
     labs( y = NULL)


```


## 2. Sentiment analysis 
Sentiment analysis is an inner_join operation.  
```{r}
# lexicons based on unigrams, i.e., single words 
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

# sentiment analysis 
tidy_books <- 
     austen_books() %>%
     group_by(book) %>%
     mutate(
          linenumber = row_number(),
          chapter = cumsum(str_detect(text, 
                                      regex("^chapter [\\divxlc]", 
                                            ignore_case = TRUE)))) %>%
     ungroup() %>%
     # use word cause stop_words and lexicons contain column `word`
     unnest_tokens(word, text) 

# filter joy words 
nrc_joy <- 
     get_sentiments("nrc") %>% 
     filter(sentiment == "joy")

# most common joy words 
tidy_books %>% 
     filter(book == "Emma") %>% 
     inner_join(nrc_joy) %>% 
     count(word, sort = TRUE)

# get sentiment scores 
tidy_books %>% 
     inner_join(get_sentiments("bing")) %>% 
     count(book, index = linenumber %/% 80, sentiment) %>% 
     pivot_wider(names_from = sentiment, 
                 values_from = n, 
                 values_fill =  0) %>% 
     mutate(sentiment = positive - negative) %>% 
     
     # plot sentiment scores 
     ggplot(aes(index, sentiment, fill = book)) +
     geom_col(show.legend = FALSE) + 
     facet_wrap( ~ book, ncol = 2, scales = "free_x")

```

2.3. Comparing sentiment dictionaries 

```{r}
# pride & prejudice book 
pride_prejudice <- 
     tidy_books %>% 
     filter(book == "Pride & Prejudice") 

# compare the three lexicons 
afinn <- 
     tidy_books %>% 
     filter(book == "Pride & Prejudice") %>% 
     inner_join(get_sentiments("afinn")) %>% 
     group_by(index = linenumber %/% 80) %>% 
     summarise(sentiment = sum(value)) %>% 
     mutate(method = "AFINN")


bing_and_nrc <- 
     bind_rows(pride_prejudice %>% 
               inner_join(get_sentiments("bing")) %>% 
               mutate(method = "Bing et al."), 
          pride_prejudice %>% 
               inner_join(get_sentiments("nrc")) %>% 
               filter(sentiment %in% c("positive", "negative"))
          ) %>% 
     mutate(method = "NRC") %>% 
     count(method, index = linenumber %/% 80, sentiment) %>% 
     pivot_wider(names_from = sentiment, 
                 values_from = n, 
                 values_fill = 0) %>% 
     mutate(sentiment = positive - negative)



bind_rows(bing_and_nrc, afinn) %>% 
     ggplot(aes(index, sentiment, fill = method)) + 
     geom_col(show.legend = FALSE) + 
     facet_wrap( ~ method, ncol =  1, scales = "free_y")
     
```

Most common and positive words 
```{r}
tidy_books %>%
     inner_join(get_sentiments("bing")) %>%
     count(word, sentiment, sort = TRUE) %>%
     ungroup() %>% 
     group_by(sentiment) %>% 
     slice_max(n, n=10) %>% 
     ungroup() %>% 
     mutate(word = reorder(word, n)) %>% 
     ggplot(aes(n, word, fill = sentiment)) + 
     geom_col(show.legend = FALSE) + 
     facet_wrap( ~ sentiment, ncol =  2, scales = "free_y")

```

2.5. Wordcloud 
```{r}
library(wordcloud)

# create a wordcloud plot (the most common words)
tidy_books  %>% 
     anti_join(stop_words) %>% 
     count(word) %>% 
     with(wordcloud(word, n, max.words = 100))
```

